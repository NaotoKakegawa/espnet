#----------------------------------------------------------
# jesc original
#----------------------------------------------------------
context-residual: true

# minibatch related
# batch-size: 1
maxlen-in: 300  # if input length  > maxlen-in, batchsize is automatically reduced
maxlen-out: 300 # if output length > maxlen-out, batchsize is automatically reduced

# other config
#dropout-rate: 0.3
lsm-weight: 0.1

# optimization related
sortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs

# scheduled sampling option
# 多分Attention画像の保存枚数
num-save-attention: 3

#----------------------------------------------------------
# copied from ljspeech tts
#----------------------------------------------------------

# network architecture
model-module: espnet.nets.pytorch_backend.e2e_mt_transformer:E2E
adim: 384
aheads: 4
elayers: 6
eunits: 1536
dlayers: 6
dunits: 1536

# minibatch related
batch-size: 8
accum-grad: 8
#batch-bins: 2277000   # 30 * (870 * 80 + 180 * 35)
                      # batch-size * (max_out * dim_out + max_in * dim_in)
                      # resuling in 38 ~ 127 samples (avg 38 samples) in batch (330 batches per epochs) for ljspeech

# training related
transformer-init: pytorch
transformer-warmup-steps: 4000
transformer-lr: 1.0

# optimization related
opt: noam
grad-clip: 1.0
weight-decay: 0.0
patience: 0
epochs: 40  # 1,000 epochs * 330 batches / 2 accum-grad = 165,000 iters
